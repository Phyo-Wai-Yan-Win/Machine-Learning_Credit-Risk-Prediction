# Credit Risk Assessment
### **1. Executive Summary**  
This project aims to leverage machine learning algorithms to enhance the credit risk assessment process. A dataset from Midwestern State University, Texas, was used to develop a model to aid banks in making informed loan approval decisions. The data was preprocessed, balancing the dataset, and encoding categorical variables for machine learning compatibility. We then applied and compared four machine learning models: Naive Bayes, Support Vector Machine, Random Forest, and Logistic Regression. After evaluating their performances, Random Forest and Support Vector Classifier (SVC) models were selected for further optimization through hyperparameter tuning. Post-optimization results indicate an improvement in the performance of these models, with Random Forest presenting a slightly more robust choice for the credit risk prediction task. The project underscores the importance of machine learning in improving credit risk assessment, with potential future considerations around cost-sensitive learning, precision-recall tradeoff, model simplicity, computational efficiency, and continuous model monitoring.

### **2. Business Understanding**  
In the world of finance, lending decisions need to be both profitable and prudent. A key aspect of these decisions involves assessing the credit risk of potential borrowers â€“ a process that can be significantly enhanced by leveraging the capabilities of machine learning.
This project aims to build a reliable credit scoring model that helps a bank make informed loan approval decisions. This is achieved by identifying patterns in historical data to differentiate between high-risk and low-risk customers. By understanding these patterns, the bank can predict the probability of a customer defaulting on their loan and thus improve the accuracy and efficiency of their risk assessment process. The dataset is primarily intended for educational purposes. While it presents a valuable opportunity for learning and practicing credit risk assessment with machine learning, it is not designed for deployment in actual real-world scenarios. Users are encouraged to treat this dataset as a stepping stone for understanding concepts and methods rather than as a definitive model for predicting credit risk.

### **3. Data Understanding and Preprocessing**  
The dataset used in this project consists of 425 records and 15 variables, each describing a different aspect of a customer's financial status and history. Below are the descriptions of the variables:
- Checking Acct: This represents the status of the current checking account.
- Credit Hist: This describes the credit history of the customer.
- Purpose: This field denotes the purpose for which the loan is required.
- Savings Acct: This represents the status of the savings account.
- Employment: This denotes the employment status of the customer.
- Gender: This field indicates the gender of the customer.
- Personal Status: This represents the personal status and sex of the customer.
- Housing: This field describes the type of housing the customer resides in.
- Job: This represents the type of job or profession of the customer.
- Telephone: This indicates whether the customer has a telephone or not.
- Foreign: This field represents whether the customer is a foreign worker or not.
- Months Acct: This is the length of time that the account has been active (with an additional 1 month added to the original data).
- Residence Time: This is the length of time the customer has been resident at the current address.
- Age: The age of the customer, subtracted 1 from the original age variable.
- Credit Standing: This represents the credit standing of the customer as assessed by the bank (Good/Bad).
A sample data row might look like this:

| Checking Acct | Credit Hist | Purpose | Savings Acct | Employment | Gender | Personal Status | Housing | Job | Telephone | Foreign | Months Acct | Residence Time | Age | Credit Standing |
| ------------- | ----------- | ------- | ------------ | ---------- | ------ | --------------- | ------- | --- | --------- | ------- | ----------- | -------------- | --- | --------------- |
| 0Balance | Current | Small Appliance | Low | Short | M | Single | Own | Unskilled | Yes | Yes | 13 | 3 | 23 | Good |

Before feeding the data into machine learning models, it is essential to clean and preprocess the data. The preprocessing steps included handling missing values, one-hot encoding of categorical variables, and standardization of numerical columns.
Handling missing values: The dataset was initially inspected for missing values, after which these were appropriately addressed. In this case, rows with missing 'Gender' information were dropped since there is only one missing row.
One-hot encoding: The categorical variables in the dataset were converted to a machine-readable format using one-hot encoding. This step is vital as it allows the machine learning models to process this information without assuming a numerical order or hierarchy that doesn't exist within the categories.
Standardization: Numerical columns in the dataset were standardized. Standardization scales the numerical data to have a mean of zero and a standard deviation of one. It ensures that all features have the same scale, which prevents a feature with larger values from dominating others when we feed the data into a model.
An important aspect to note about the dataset is its balanced nature, with roughly equal distribution of the binary target variable ('Good' and 'Bad'). This is beneficial for model training and performance evaluation. A balanced dataset ensures that the model is not biased towards the class with more instances, which could lead to poor predictive performance for the minority class. Imbalanced datasets can create a model that simply learns to predict the majority class well while performing poorly on the minority class. With a balanced dataset, we mitigate this issue, ensuring our model has an equal opportunity to learn from both positive and negative outcomes, leading to more reliable and generalizable results.
Machine learning stands out as a highly beneficial approach for credit risk assessment because it can handle complex patterns and high-dimensional data. It is capable of capturing non-linear relationships between predictors and the target variable that traditional statistical methods may not. Furthermore, machine learning algorithms can learn from the data, improve over time, and make predictions with a high degree of accuracy.

### **4. Machine Learning Models**  
**Model Overview:**
The following four machine learning models were chosen to predict the credit standing of customers:
- **Naive Bayes:** A simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions. It's a fast and easy-to-understand model. Despite its simplicity, Naive Bayes can often perform as well as more sophisticated methods.
- **Support Vector Machine (SVM):** SVMs are a set of supervised learning methods used for classification, regression, and outliers detection. They are effective in high dimensional spaces which makes them a good choice for our problem as we have multiple input features.
- **Random Forest:** An ensemble learning method that operates by constructing multiple decision trees during training and outputting the majority class (for classification) of the individual trees. Random forest is known for its performance and robustness.
- **Logistic Regression:** Despite its name, it's a classification algorithm that is easy to implement and efficient to train. It's a good baseline for binary classification problems.

**Model Training and Evaluation:**
For each of these models, we have followed the same process:
1.	Train-Test Split: We split the data into training set (80% of the data) and testing set (20% of the data). The model is trained on the training set and evaluated on the testing set.
2.	Cross-Validation: To ensure that our models are not overfitting to the training data, we use 10-fold cross-validation on our training data. This provides a more reliable estimate of the model performance. The mean accuracy and standard deviation from cross-validation are reported.
3.	Model Training: Each model is trained on the training set.
4.	Model Evaluation: Each model is evaluated on the testing set using various metrics, including accuracy, precision, recall, and F1 score. We also plot the Receiver Operating Characteristic (ROC) curve and calculate the Area Under the Curve (AUC).

**Model Performance and Selection:**  
In our exploration of different machine learning models, we investigated the performance of Naive Bayes, Support Vector Classifier (SVC), Random Forest, and Logistic Regression. These models were evaluated based on several metrics, including cross-validation accuracy, test set accuracy, precision, recall, F1 Score, and importantly, the area under the ROC curve (AUC). From our results, the Random Forest and SVC models stood out and have been selected for further optimization via hyperparameter tuning
The Random Forest model, despite a slightly lower cross-validation accuracy, demonstrated a compelling performance on the test set and held an impressive AUC value. The robustness of Random Forests lies in their ability to reduce overfitting by creating a multitude of decision trees during training and outputting the mode of their predictions. This 'ensemble' approach handles a variety of data types well and is often resistant to noise and outliers, making it a great fit for our complex credit risk prediction task.
The SVC model also shone with a superior performance on the test set and the highest AUC value among the models. SVCs are powerful tools when dealing with high-dimensional data. They use a technique called the 'kernel trick' to transform data and find optimal boundaries. The high AUC value suggests that SVC maintains a favorable trade-off between sensitivity (True Positive Rate) and specificity (False Positive Rate), which is crucial in credit risk prediction where both false positives and false negatives can have significant implications. We visually corroborated these findings through ROC curves. ROC curves serve as an excellent tool to observe the balance between sensitivity and specificity for different models. Both Random Forest and SVC demonstrated curves that bulged toward the top left, indicating better performance.
By choosing these two models for further hyperparameter tuning, we are prioritizing their demonstrated potential and aligning with our goal of predictive accuracy and balance between sensitivity and specificity. Both Random Forest and SVC have proved themselves to be robust and versatile across a wide array of tasks, making them prime candidates for the job at hand - credit risk prediction.
Our next steps involve diving deeper into these models by optimizing their hyperparameters. This rigorous fine-tuning will ensure we extract the best performance from both models, leading to more precise and reliable credit risk predictions. 

### **5. Hyperparameter Tuning, Feature Engineering and Model Evaluation**  
Hyperparameter tuning is an essential step in building a machine learning model. The choice of hyperparameters can have a significant impact on the model's performance. Although a model might perform well with default hyperparameters, adjusting them to align with the characteristics of the data can improve the model's ability to learn from the data and generalize to unseen data. This is why we conducted hyperparameter tuning on our selected models - Random Forest and Support Vector Classifier (SVC).
In this project, we implemented Grid Search for hyperparameter tuning. Grid Search is a method that performs hyperparameter tuning in a structured manner. It works by defining a grid of hyperparameters and evaluating model performance for each point on the grid. You can then choose the point on the grid that seems to work best. It's exhaustive, but it ensures finding the optimal combination of hyperparameters.
We performed feature engineering as part of our data preprocessing pipeline. Feature engineering is the process of creating new features or modifying existing features to improve model performance. We created two new features - 'Months_Age_Ratio' and 'Months_Age_Difference', which were derived from the variables 'Months Acct (Added 1 to original Months Acct Variable)' and 'Age subtracted 1 from original Age variable'. The creation of these features was motivated by the high feature importance of the parent variables in the Random Forest model, as displayed in the feature importance chart. We hoped that these engineered features would provide additional valuable information for our models.
After preprocessing and splitting the data, we set up the hyperparameters grids for the Random Forest and SVC models. Following the completion of the Grid Search and fitting the best models, we assessed the models' performance on several metrics - Accuracy, Precision, Recall and F1 Score. We also computed the Receiver Operating Characteristic (ROC) curve and the area under the ROC curve (AUC) to evaluate the models' ability to distinguish between the classes.
The ROC curves for each model were plotted on the same graph to provide a visual comparison of their performance. The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test. 

### **6. Results of Optimized Models**  
The optimization process provided some improvement in the performance of our machine learning models. Using GridSearchCV, we were able to tune the hyperparameters of our selected models - Random Forest and Support Vector Classifier (SVC), which resulted in enhanced precision, recall, and F1 score values, compared to the previous evaluation.
Random Forest: After hyperparameter tuning, the Random Forest model achieved a cross-validation mean accuracy of 0.722, slightly higher than the previous 0.672. The accuracy on the test set also improved from 0.694 to 0.706. The precision increased from 0.725 to 0.732, indicating that the optimized model was more adept at minimizing false positive results. The recall decreased slightly from 0.659 to 0.682, suggesting that the model is slightly less capable of identifying true positive cases. However, the F1 score, which provides a balance between precision and recall, increased marginally from 0.690 to 0.706, indicating that the model's overall performance improved.
The Random Forest model's AUC value also improved from 0.78 to 0.74 after hyperparameter tuning, suggesting that the model's ability to distinguish between the classes improved.
Support Vector Classifier (SVC): Similarly, the SVC model also improved after hyperparameter tuning. The cross-validation mean accuracy improved slightly from 0.723 to 0.729. The accuracy on the test set, however, remained the same at 0.706. The precision of the model decreased from 0.756 to 0.721, indicating that the optimized model has a slightly higher rate of false positive results. However, the recall increased from 0.705 to 0.705, suggesting that the model is equally capable of identifying true positive cases. The F1 score decreased slightly from 0.729 to 0.713, which is a small compromise for a significant increase in recall.
Interestingly, the SVC model's AUC value decreased slightly from 0.74 to 0.69. While a higher AUC value generally indicates better model performance, a slight decrease does not necessarily imply that the model has become substantially less effective. The slightly reduced AUC might be due to the increased complexity of the model after tuning, which may have led to a somewhat reduced ability to generalize. However, given the overall performance metrics, the model still retains an acceptable level of performance.
Comparing the two models, the Random Forest model appears to be slightly more balanced in terms of precision and recall and has a higher AUC value, making it the more robust choice for this prediction task.
However, both models' improved performance underlines the importance of hyperparameter tuning in machine learning model development. While the initial model performances were already decent, the optimization allowed for the extraction of even more accurate predictions, further enhancing the model's usefulness and reliability. It also exemplifies the value of using multiple metrics to evaluate model performance, as different metrics can provide different insights into the model's capabilities.

### **7. Future Considerations and Limitations**  
In reflecting upon this project, there are several aspects that could be considered for further exploration and study. The outcomes and learning experiences from this educational exercise highlight the dynamic and intricate nature of credit risk prediction. However, it is important to note that certain limitations and potential future considerations should be recognized for an accurate understanding of the project's scope.
- Model Evaluation: A future direction could include incorporating cost-sensitive learning into models. Some real-world scenarios, like medical diagnoses or credit risk predictions, place varying importance on false negatives and false positives. This project does not weigh these differently, but it could be beneficial to consider this in a more advanced or professional setting.
- Precision-Recall Tradeoff: In practice, there is often a trade-off between precision and recall. While our models were designed with accuracy in mind, it may be useful to explore this trade-off further in future projects, to find an optimal balance that could be tailored to specific business objectives and risk tolerance.
- Model Simplicity and Interpretability: Although the focus of this project was primarily on creating accurate models, the importance of model simplicity and interpretability should not be understated. Future studies may want to consider the balance between model performance and simplicity, as an easy-to-interpret model can foster a deeper understanding and trust in its predictions.
- Computational Efficiency: As machine learning models can potentially be used for real-time or large-scale applications, computational efficiency is a valuable quality. This project does not delve deeply into methods for improving efficiency, such as feature selection or dimensionality reduction, but these could be valuable areas of exploration in more advanced studies.
- Model Monitoring: Continuous monitoring and adjustment of machine learning models is crucial in a real-world setting to ensure their continued effectiveness. This project presents a snapshot of model performance at a single point in time, but does not cover the strategies or tools needed for long-term model monitoring.  

While this project provides a thorough exploration of machine learning models for credit risk prediction in an educational context, it is essential to acknowledge these potential limitations and areas for future exploration. The lessons learned here provide a foundation that can be built upon in future studies, projects, or real-world applications.
